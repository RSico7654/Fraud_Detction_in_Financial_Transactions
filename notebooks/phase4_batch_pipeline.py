# -*- coding: utf-8 -*-
"""phase4_batch_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wxjmTnnzSeCb5AF0FGBWJnH59AbAsFvs
"""

# Commented out IPython magic to ensure Python compatibility.

# Clone repo into Colab
!git clone https://github.com/RSico7654/Fraud_Detction_in_Financial_Transactions.git

# Check you are inside repo
# %cd /content/Fraud_Detction_in_Financial_Transactions

# =========================================
# Fraud Detection Project - Dataset Download
# =========================================

# Step 1: Upload Kaggle API key
from google.colab import files
files.upload()  # <-- Upload kaggle.json here

# Step 2: Setup Kaggle directory
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Step 3: Download dataset (Credit Card Fraud)
!kaggle datasets download -d mlg-ulb/creditcardfraud

# Step 4: Unzip into /data folder
!mkdir -p data
!unzip -o creditcardfraud.zip -d data/

# Step 5: Load dataset with pandas
import pandas as pd

df = pd.read_csv("data/creditcard.csv")
print("Shape of dataset:", df.shape)

# Preview first rows
df.head()

import pandas as pd

# Load dataset (already downloaded using kaggle.json earlier)
df = pd.read_csv("data/creditcard.csv")
print("Dataset shape:", df.shape)
df.head()

# Check missing values
print(df.isnull().sum())

# Drop missing values if any
df = df.dropna()

print("After dropping missing values:", df.shape)

from sklearn.preprocessing import StandardScaler

df['normalized_amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))
df['normalized_time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))

# Drop original columns
df = df.drop(['Amount', 'Time'], axis=1)
df.head()

from imblearn.over_sampling import SMOTE

X = df.drop('Class', axis=1)
y = df['Class']

print("Before SMOTE:", y.value_counts())

# Oversample fraud cases to balance dataset
smote = SMOTE(sampling_strategy=0.5, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("After SMOTE:", y_resampled.value_counts())

preprocessed_df = pd.DataFrame(X_resampled, columns=X.columns)
preprocessed_df['Class'] = y_resampled

# Save into /data folder
preprocessed_df.to_csv("data/creditcard_preprocessed.csv", index=False)

print("Preprocessed dataset saved:", preprocessed_df.shape)

from sklearn.model_selection import train_test_split

X = df.drop("Class", axis=1)
y = df["Class"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train:", X_train.shape, "Test:", X_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(64,32), max_iter=100)
}

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])
    results[name] = auc

    print(f"\n{name}")
    print("AUC-ROC:", auc)
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))

print("\nSummary of AUC Scores:", results)

from sklearn.model_selection import GridSearchCV

param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [4, 6, 8]
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring="roc_auc")
grid.fit(X_train, y_train)

print("Best Params:", grid.best_params_)
print("Best ROC AUC:", grid.best_score_)

import joblib

best_model = RandomForestClassifier(**grid.best_params_)
best_model.fit(X_train, y_train)

# Save model
joblib.dump(best_model, "models/best_model.pkl")

import pandas as pd

# Simulating a SQL fetch by reading CSV
batch_data = pd.read_csv("data/creditcard_preprocessed.csv").sample(500, random_state=42)
print("Batch data shape:", batch_data.shape)
batch_data.head()

import joblib

model = joblib.load("models/best_model.pkl")
print("Loaded model successfully!")

X_batch = batch_data.drop("Class", axis=1)
y_batch = batch_data["Class"]

batch_data["Prediction"] = model.predict(X_batch)
batch_data["Fraud_Probability"] = model.predict_proba(X_batch)[:,1]

batch_data.head()

batch_data.to_csv("data/batch_predictions.csv", index=False)
print("Predictions saved to data/batch_predictions.csv")

!pip install schedule

import schedule
import time
import pandas as pd
import joblib
import os
from datetime import datetime

# Load model once
model = joblib.load("models/best_model.pkl")

# Paths
INPUT_FILE = "data/creditcard_preprocessed.csv"   # must stay clean
OUTPUT_FILE = "data/batch_predictions.csv"

def batch_job():
    print("Running batch job...")

    # ✅ Always load clean dataset
    df = pd.read_csv(INPUT_FILE).sample(500, random_state=None)

    # ✅ Ensure only training features are used
    feature_cols = [col for col in df.columns if col not in ["Class", "Prediction", "Fraud_Probability"]]
    X = df[feature_cols]

    # Predict
    df["Prediction"] = model.predict(X)
    df["Fraud_Probability"] = model.predict_proba(X)[:, 1]
    df["Timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ✅ Append to output file (never overwrite input!)
    if not os.path.exists(OUTPUT_FILE):
        df.to_csv(OUTPUT_FILE, index=False)
    else:
        df.to_csv(OUTPUT_FILE, mode="a", header=False, index=False)

    print("Batch job done. Appended predictions.")

# Demo loop (replace scheduler for testing in Colab)
for i in range(3):
    batch_job()
    time.sleep(5)